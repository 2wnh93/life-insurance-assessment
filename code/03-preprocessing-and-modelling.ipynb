{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Modelling\n",
    "\n",
    "In this notebook, I preprocess the data (balance the classes, and scale) and proceed to apply the following algorithms : \n",
    "\n",
    "- Support Vector Classifier\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "\n",
    "**Contents:**\n",
    "- [Import libraries and data](#Import-libraries-and-data)\n",
    "- [Preprocessing](#Preprocessing)\n",
    "- [Modelling](#Modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from prettytable import PrettyTable\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# modelling\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"All imported successfully!\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train = pd.read_csv(\"../data/train-reviewed.csv\")\n",
    "test = pd.read_csv(\"../data/test-clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Here I split the training set to get a validation set, \n",
    "balance the classes using `SMOTE` and then scale the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features and target variable separately\n",
    "features = [col for col in train.columns if col != 'Response']\n",
    "X = train[features]\n",
    "y = train['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split with test size 20%\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balance classes with SMOTE\n",
    "By default, `SMOTE` will oversample all classes to have the same number of samples as the class with the most samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count before SMOTE:  Counter({8: 14587, 6: 8401, 7: 6005, 2: 4898, 1: 4642, 5: 4067, 4: 1070, 3: 757})\n",
      "Count after SMOTE:  Counter({6: 14587, 8: 14587, 4: 14587, 2: 14587, 7: 14587, 5: 14587, 1: 14587, 3: 14587})\n"
     ]
    }
   ],
   "source": [
    "# apply SMOTE\n",
    "smote = SMOTE()\n",
    "counter_before = Counter(y_train)\n",
    "print(\"Count before SMOTE: \", counter_before)\n",
    "\n",
    "#fit and resample with SMOTE\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train,y_train)\n",
    "\n",
    "counter_after = Counter(y_train_sm)\n",
    "print(\"Count after SMOTE: \", counter_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Data\n",
    "Since some data has already been normalized (ranging between 0 and 1), I will use `MinMaxScaler` to scale other features to have the same bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "#fit and transform train set\n",
    "X_train_sm_sc = mms.fit_transform(X_train_sm)\n",
    "\n",
    "#transform the validation and test sets\n",
    "X_val_sc = mms.transform(X_val)\n",
    "test_sc = mms.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "I apply the following classification algorithms in a pipeline and determine which one achieves the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate classifiers\n",
    "rfc = RandomForestClassifier()\n",
    "svc = SVC()\n",
    "xgbc = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "p1 = Pipeline([('mm', MinMaxScaler()),\n",
    "              ('rfc', rfc)])\n",
    "p2 = Pipeline([('mm', MinMaxScaler()),\n",
    "              ('svc', svc)])\n",
    "p3 = Pipeline([('mm', MinMaxScaler()),\n",
    "              ('xgbc', xgbc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "params1 = [{'rfc__n_estimators': [100, 250, 500], \n",
    "           'rfc__max_depth': [10,20],\n",
    "           'rfc__min_samples_split': [5,10,20]}]\n",
    "params2 = [{'svc__C': [1], \n",
    "           'svc__kernel': ['linear']}] \n",
    "params3 = [{'xgbc__n_estimators': [50, 100, 250, 500],\n",
    "           'xgbc__max_depth': [3,5,10],\n",
    "           'xgbc__learning_rate': [0.05,0.1,0.3]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up gridsearch for each algo\n",
    "gridcvs = {}\n",
    "\n",
    "# set folds for inner\n",
    "inner_cv = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# make scorer\n",
    "kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "\n",
    "for paramgrid, estimator, name in zip((params1,params2,params3),\n",
    "                                     (p1,p2,p3),\n",
    "                                     ('Random Forest Classifier', 'Support Vector Classifier', 'XGBoost Classifier')):\n",
    "    gcv = GridSearchCV(estimator = estimator,\n",
    "                      param_grid = paramgrid,\n",
    "                      scoring = kappa_scorer,\n",
    "                      n_jobs=-1,\n",
    "                      cv=inner_cv,\n",
    "                      verbose=0,\n",
    "                      refit=True)\n",
    "    gridcvs[name]=gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# score on algos\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# create table\n",
    "a = PrettyTable(title=\"Cross-validated ROC-AUC score\", header_style='title', max_table_width=110)\n",
    "a.field_names =[\"Algorithms\", \"ROC-AUC score\", \"Standard Deviation\"]\n",
    "\n",
    "# get cross val score \n",
    "for name, gs_est in sorted(gridcvs.items()):\n",
    "    nested_score = cross_val_score(gs_est,\n",
    "                                  X=X_train_sm_sc,\n",
    "                                  y=y_train_sm,\n",
    "                                  cv=outer_cv,\n",
    "                                  scoring=kappa_scorer)\n",
    "    a.add_row([name, f'{round(nested_score.mean(),3)*100}%', f'+/- {round(nested_score.std(),3)*100}%'])\n",
    "    print(f'Done with {name}')\n",
    "\n",
    "#print table\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
