{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Modelling\n",
    "\n",
    "In this notebook, I preprocess the data (balance the classes, and scale) and proceed to apply the following algorithms : \n",
    "\n",
    "- Support Vector Classifier\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "\n",
    "**Contents:**\n",
    "- [Import libraries and data](#Import-libraries-and-data)\n",
    "- [Preprocessing](#Preprocessing)\n",
    "- [Modelling](#Modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from prettytable import PrettyTable\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# modelling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.metrics import cohen_kappa_score \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"All imported successfully!\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train = pd.read_csv(\"../data/train-clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Here I split the training set to get a validation set, \n",
    "balance the classes using `SMOTE` and then scale the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features and target variable separately\n",
    "features = [col for col in train.columns if col != 'Response']\n",
    "X = train[features]\n",
    "y = train['Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split with test size 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balance classes with SMOTE\n",
    "By default, `SMOTE` will oversample all classes to have the same number of samples as the class with the most samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "counter_before = Counter(y_train)\n",
    "print(\"Count before SMOTE: \", counter_before)\n",
    "\n",
    "#fit and resample with SMOTE\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train,y_train)\n",
    "\n",
    "counter_after = Counter(y_train_ada)\n",
    "print(\"Count after SMOTE: \", counter_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Data\n",
    "Since some data has already been normalized (ranging between 0 and 1), I will use `MinMaxScaler` to scale other features to have the same bounds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "I apply the following classification algorithms in a pipeline and determine which one achieves the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate classifiers\n",
    "rfc = RandomForestClassifier()\n",
    "svc = SVC()\n",
    "xgbc = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "p1 = Pipeline([('mm', MinMaxScaler()),\n",
    "              ('rfc', rfc)])\n",
    "p2 = Pipeline([('mm', MinMaxScaler()),\n",
    "              ('svc', svc)])\n",
    "p3 = Pipeline([('mm', MinMaxScaler()),\n",
    "              ('xgbc', xgbc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "params1 = [{'rfc__n_estimators': [100, 250, 500], \n",
    "           'rfc__max_depth': [10,20],\n",
    "           'rfc__min_samples_split': [5,10,20]}]\n",
    "params2 = [{'svc__C': [1], \n",
    "           'svc__kernel': ['linear']}] \n",
    "params3 = [{'xgbc__n_estimators': [50, 100, 250, 500],\n",
    "           'xgbc__max_depth': [3,5,10],\n",
    "           'xgbc__learning_rate': [0.05,0.1,0.3]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up gridsearch for each algo\n",
    "gridcvs = {}\n",
    "\n",
    "inner_cv = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "for paramgrid, estimator, name in zip((params1,params2,params3),\n",
    "                                     (p1,p2,p3),\n",
    "                                     ('Random Forest Classifier', 'Support Vector Classifier', 'XGBoost Classifier')):\n",
    "    gcv = GridSearchCV(estimator = estimator,\n",
    "                      param_grid = paramgrid,\n",
    "                      scoring = 'cohen_kappa_score',\n",
    "                      n_jobs=-1,\n",
    "                      cv=inner_cv,\n",
    "                      verbose=0,\n",
    "                      refit=True)\n",
    "    gridcvs[name]=gcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# score on algos\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "  \n",
    "a = PrettyTable(title=\"Cross-validated ROC-AUC score\", header_style='title', max_table_width=110)\n",
    "a.field_names =[\"Algorithms\", \"ROC-AUC score\", \"Standard Deviation\"]\n",
    "for name, gs_est in sorted(gridcvs.items()):\n",
    "    nested_score = cross_val_score(gs_est,\n",
    "                                  X=X_train_ada,\n",
    "                                  y=y_train_ada,\n",
    "                                  cv=outer_cv,\n",
    "                                  scoring='cohen_kappa_score')\n",
    "    a.add_row([name, f'{round(nested_score.mean(),3)*100}%', f'+/- {round(nested_score.std(),3)*100}%'])\n",
    "    print(f'Done with {name}')\n",
    "#print table\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
